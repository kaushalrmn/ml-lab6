{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Assignment 9\n",
    "#### Student Name: Ramandeep Kaur\n",
    "#### ID: 8976849"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sample sentences above. You are required for this assignment to implement four functions **from scratch**. <br>\n",
    "You are required to preprocess the text and apply the tokenization process as done in assignment 8. (3)\n",
    "***THEN***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'versatile', 'programming', 'language']\n",
      "['javascript', 'widely', 'used', 'web', 'development']\n",
      "['java', 'known', 'platform', 'independence']\n",
      "['programming', 'involves', 'writing', 'code', 'solve', 'problems']\n",
      "['data', 'structures', 'crucial', 'efficient', 'programming']\n",
      "['algorithms', 'stepbystep', 'instructions', 'solving', 'problems']\n",
      "['version', 'control', 'systems', 'help', 'manage', 'code', 'changes', 'collaboration']\n",
      "['debugging', 'process', 'finding', 'fixing', 'errors', 'code']\n",
      "['web', 'frameworks', 'simplify', 'development', 'web', 'applications']\n",
      "['artificial', 'intelligence', 'applied', 'various', 'programming', 'tasks']\n"
     ]
    }
   ],
   "source": [
    "## TODO: Clean the sentences\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(sentence):\n",
    "    # Remove punctuation\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # Tokenize words\n",
    "    words = word_tokenize(sentence)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Preprocess and tokenize each sentence\n",
    "tokenized_sentences = [preprocess_text(sentence) for sentence in sample_sentences]\n",
    "\n",
    "# Print tokenized sentences\n",
    "for tokens in tokenized_sentences:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving python dictionary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the inverted index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: [1]\n",
      "is: [1, 2, 3, 8]\n",
      "a: [1]\n",
      "versatile: [1]\n",
      "programming: [1, 10]\n",
      "language.: [1]\n",
      "JavaScript: [2]\n",
      "widely: [2]\n",
      "used: [2]\n",
      "for: [2, 3, 5, 6]\n",
      "web: [2, 9]\n",
      "development.: [2]\n",
      "Java: [3]\n",
      "known: [3]\n",
      "its: [3]\n",
      "platform: [3]\n",
      "independence.: [3]\n",
      "Programming: [4]\n",
      "involves: [4]\n",
      "writing: [4]\n",
      "code: [4, 7]\n",
      "to: [4]\n",
      "solve: [4]\n",
      "problems.: [4, 6]\n",
      "Data: [5]\n",
      "structures: [5]\n",
      "are: [5, 6]\n",
      "crucial: [5]\n",
      "efficient: [5]\n",
      "programming.: [5]\n",
      "Algorithms: [6]\n",
      "step-by-step: [6]\n",
      "instructions: [6]\n",
      "solving: [6]\n",
      "Version: [7]\n",
      "control: [7]\n",
      "systems: [7]\n",
      "help: [7]\n",
      "manage: [7]\n",
      "changes: [7]\n",
      "in: [7, 8, 10]\n",
      "collaboration.: [7]\n",
      "Debugging: [8]\n",
      "the: [8, 9]\n",
      "process: [8]\n",
      "of: [8, 9]\n",
      "finding: [8]\n",
      "and: [8]\n",
      "fixing: [8]\n",
      "errors: [8]\n",
      "code.: [8]\n",
      "Web: [9]\n",
      "frameworks: [9]\n",
      "simplify: [9]\n",
      "development: [9]\n",
      "applications.: [9]\n",
      "Artificial: [10]\n",
      "intelligence: [10]\n",
      "can: [10]\n",
      "be: [10]\n",
      "applied: [10]\n",
      "various: [10]\n",
      "tasks.: [10]\n"
     ]
    }
   ],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the inverted index \n",
    "\n",
    "    inverted_index = {}\n",
    "    for sentence_id, sentence in enumerate(list_of_sentences, start=1):\n",
    "        tokens = sentence.split()  # Tokenize the sentence\n",
    "        for token in tokens:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = {sentence_id}\n",
    "            else:\n",
    "                inverted_index[token].add(sentence_id)\n",
    "    return inverted_index\n",
    "\n",
    "# List of sentences\n",
    "list_of_sentences = [\n",
    "    \"Python is a versatile programming language.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]\n",
    "\n",
    "inverted_index = get_inverted_index(list_of_sentences)\n",
    "\n",
    "# Output the inverted index\n",
    "for token, sentences in inverted_index.items():\n",
    "    print(f\"{token}: {sorted(sentences)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the Positional index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "    Python: [(1, 0)]\n",
      "    is: [(1, 1), (2, 1), (3, 1)]\n",
      "    a: [(1, 2)]\n",
      "    versatile: [(1, 3)]\n",
      "    programming: [(1, 4)]\n",
      "    language: [(1, 5)]\n",
      "\n",
      "Sentence 2:\n",
      "    JavaScript: [(2, 0)]\n",
      "    is: [(1, 1), (2, 1), (3, 1)]\n",
      "    widely: [(2, 2)]\n",
      "    used: [(2, 3)]\n",
      "    for: [(2, 4), (3, 3)]\n",
      "    web: [(2, 5)]\n",
      "    development: [(2, 6)]\n",
      "\n",
      "Sentence 3:\n",
      "    Java: [(3, 0)]\n",
      "    is: [(1, 1), (2, 1), (3, 1)]\n",
      "    known: [(3, 2)]\n",
      "    for: [(2, 4), (3, 3)]\n",
      "    its: [(3, 4)]\n",
      "    platform: [(3, 5)]\n",
      "    independence: [(3, 6)]\n",
      "\n",
      "{'Python': [(1, 0)], 'is': [(1, 1), (2, 1), (3, 1)], 'a': [(1, 2)], 'versatile': [(1, 3)], 'programming': [(1, 4)], 'language': [(1, 5)], 'JavaScript': [(2, 0)], 'widely': [(2, 2)], 'used': [(2, 3)], 'for': [(2, 4), (3, 3)], 'web': [(2, 5)], 'development': [(2, 6)], 'Java': [(3, 0)], 'known': [(3, 2)], 'its': [(3, 4)], 'platform': [(3, 5)], 'independence': [(3, 6)]}\n"
     ]
    }
   ],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the positional index\n",
    "\n",
    "    positional_index_dict = {}\n",
    "    for sentence_id, sentence in enumerate(list_of_sentence_tokens, start=1):\n",
    "        for position, token in enumerate(sentence):\n",
    "            if token not in positional_index_dict:\n",
    "                positional_index_dict[token] = []\n",
    "            positional_index_dict[token].append((sentence_id, position))\n",
    "    return positional_index_dict\n",
    "\n",
    "# list_of_sentence_tokens \n",
    "list_of_sentence_tokens = [\n",
    "    [\"Python\", \"is\", \"a\", \"versatile\", \"programming\", \"language\"],\n",
    "    [\"JavaScript\", \"is\", \"widely\", \"used\", \"for\", \"web\", \"development\"],\n",
    "    [\"Java\", \"is\", \"known\", \"for\", \"its\", \"platform\", \"independence\"],\n",
    "    # Add more sentences here if needed\n",
    "]\n",
    "\n",
    "\n",
    "positional_index_dict = get_positional_index(list_of_sentence_tokens)\n",
    "\n",
    "# Print the positional index sentence-wise\n",
    "for sentence_id, sentence in enumerate(list_of_sentence_tokens, start=1):\n",
    "    print(f\"Sentence {sentence_id}:\")\n",
    "    for token in sentence:\n",
    "        print(f\"    {token}: {positional_index_dict.get(token, [])}\")\n",
    "    print()\n",
    "\n",
    "positional_index_dict = get_positional_index(list_of_sentence_tokens)\n",
    "print(positional_index_dict)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the TF-IDF Matrix that is sufficient to represent the documents. Assume that each sentence is a document and the sentence ID starts from 1. (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.35013871 0.26628951 0.35013871 0.35013871\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.35013871 0.26628951 0.         0.35013871 0.         0.\n",
      "  0.         0.35013871 0.         0.35013871 0.        ]\n",
      " [0.32767345 0.         0.         0.24920411 0.         0.\n",
      "  0.32767345 0.         0.32767345 0.32767345 0.32767345 0.\n",
      "  0.         0.24920411 0.32767345 0.         0.24920411 0.32767345\n",
      "  0.         0.         0.24920411 0.         0.        ]\n",
      " [0.         0.33046705 0.         0.         0.         0.\n",
      "  0.         0.33046705 0.         0.         0.         0.33046705\n",
      "  0.         0.         0.         0.         0.25132871 0.\n",
      "  0.33046705 0.         0.25132871 0.         0.6609341 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_tfidf_matrix(list_of_sentence_tokens):\n",
    "   # Concatenate each sentence's tokens into a single string\n",
    "    preprocessed_sentences = [' '.join(sentence) for sentence in list_of_sentence_tokens]\n",
    "\n",
    "   # Initialize TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "   # Fit the TF-IDF vectorizer to the preprocessed sentences and transform them into a TF-IDF matrix\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_sentences)\n",
    "\n",
    "    return tfidf_matrix\n",
    "\n",
    "list_of_sentence_tokens = [\n",
    "    # add sentences\n",
    "    [\"Version\", \"control\", \"systems\", \"help\", \"manage\", \"code\", \"changes\", \"in\", \"collaboration\"],\n",
    "    [\"Debugging\", \"is\", \"the\", \"process\", \"of\", \"finding\", \"and\", \"fixing\", \"errors\", \"in\", \"code\"],\n",
    "    [\"Web\", \"frameworks\", \"simplify\", \"the\", \"development\", \"of\", \"web\", \"applications\"]\n",
    "]\n",
    "\n",
    "tfidf_matrix = get_tfidf_matrix(list_of_sentence_tokens)\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 4: Create a method that takes as an input: (10)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Documents based on tfidf method and query ['sentence'] : [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def get_ranked_documents(list_of_sentence_tokens, method_name, search_query):\n",
    "    # TODO: Implement the functionality that returns the rank of the documents based on the method given and the search query\n",
    "    ## If the method is \"inverted\" then rank the documents based on the number of matching tokens \n",
    "    ## If the method is \"tfidf\" then use the tfidf score equation in slides and return ranking based on the score\n",
    "    ## The document with highest relevance should be ranked first\n",
    "    ## list method should return the index of the documents based on highest ranking first\n",
    "\n",
    "\n",
    "    # Function to calculate TF-IDF score for a term in a document\n",
    "    def calculate_tfidf(term, document, documents):\n",
    "        # Calculate term frequency (TF)\n",
    "        tf = document.count(term) / len(document)\n",
    "        # Calculate inverse document frequency (IDF)\n",
    "        idf = math.log(len(documents) / (sum(1 for doc in documents if term in doc) + 1))\n",
    "        return tf * idf\n",
    "\n",
    "    # Function to calculate inverted index score for a document\n",
    "    def calculate_inverted_index(query, document):\n",
    "        return sum(1 for term in query if term in document)\n",
    "\n",
    "    # Calculate scores based on the method\n",
    "    scores = []\n",
    "    if method_name == \"inverted\":\n",
    "        # Calculate scores using inverted index method\n",
    "        for doc in list_of_sentence_tokens:\n",
    "            scores.append(calculate_inverted_index(search_query, doc))\n",
    "    elif method_name == \"tfidf\":\n",
    "        # Calculate scores using TF-IDF method\n",
    "        for doc in list_of_sentence_tokens:\n",
    "            tfidf_score = sum(calculate_tfidf(term, doc, list_of_sentence_tokens) for term in search_query)\n",
    "            scores.append(tfidf_score)\n",
    "\n",
    "    # Rank the documents based on scores\n",
    "    ranked_documents = sorted(range(len(list_of_sentence_tokens)), key=lambda i: scores[i], reverse=True)\n",
    "    return ranked_documents\n",
    "\n",
    "sentences = [\n",
    "    [\"Algorithms\", \"are\", \"step-by-step\", \"instructions\", \"for\", \"solving\", \"problems\"],\n",
    "    [\"Version\", \"control\", \"systems\", \"help\", \"manage\", \"code\", \"changes\", \"in\", \"collaboration\"],\n",
    "    [\"Debugging\", \"is\", \"the\", \"process\", \"of\", \"finding\", \"and\", \"fixing\", \"errors\", \"in\", \"code\"],\n",
    "    [\"Web\", \"frameworks\", \"simplify\", \"the\", \"development\", \"of\", \"web\", \"applications\"],\n",
    "    [\"Artificial\", \"intelligence\", \"can\", \"be\", \"applied\", \"in\", \"various\", \"programming\", \"tasks\"],\n",
    "]\n",
    "\n",
    "\n",
    "query = [\"sentence\"]\n",
    "method = \"tfidf\"\n",
    "\n",
    "ranked_docs = get_ranked_documents(sentences, method, query)\n",
    "print(\"Ranked Documents based on\", method, \"method and query\", query, \":\", ranked_docs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
